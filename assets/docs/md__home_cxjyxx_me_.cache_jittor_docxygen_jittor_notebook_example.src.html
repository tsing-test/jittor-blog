<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.11"/>
<title>Jittor: Example: Model definition and training</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
  $(document).ready(function() { init_search(); });
</script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">Jittor
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.11 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li><a href="index.html"><span>Main&#160;Page</span></a></li>
      <li class="current"><a href="pages.html"><span>Related&#160;Pages</span></a></li>
      <li><a href="annotated.html"><span>Classes</span></a></li>
      <li><a href="files.html"><span>Files</span></a></li>
      <li>
        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="search/mag_sel.png"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.png" alt=""/></a>
          </span>
        </div>
      </li>
    </ul>
  </div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">Example: Model definition and training </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>The following example shows how to model a two-layer neural network step by step and train from scratch In a few lines of Python code.</p>
<div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;import jittor as jt</div><div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;import numpy as np</div><div class="line"><a name="l00003"></a><span class="lineno">    3</span>&#160;from jittor import nn, Module, init</div></div><!-- fragment --><p>The following code defines our model, which is a two-layer neural network. The size of hidden layer is 10. and the activation function is relu.</p>
<div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;### model define</div><div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;</div><div class="line"><a name="l00003"></a><span class="lineno">    3</span>&#160;class Model(Module):</div><div class="line"><a name="l00004"></a><span class="lineno">    4</span>&#160;    def __init__(self):</div><div class="line"><a name="l00005"></a><span class="lineno">    5</span>&#160;        self.layer1 = nn.Linear(1, 10)</div><div class="line"><a name="l00006"></a><span class="lineno">    6</span>&#160;        self.relu = nn.ReLU()</div><div class="line"><a name="l00007"></a><span class="lineno">    7</span>&#160;        self.layer2 = nn.Linear(10, 1)</div><div class="line"><a name="l00008"></a><span class="lineno">    8</span>&#160;    def execute (self,x) :</div><div class="line"><a name="l00009"></a><span class="lineno">    9</span>&#160;        x = self.layer1(x)</div><div class="line"><a name="l00010"></a><span class="lineno">   10</span>&#160;        x = self.relu(x)</div><div class="line"><a name="l00011"></a><span class="lineno">   11</span>&#160;        x = self.layer2(x)</div><div class="line"><a name="l00012"></a><span class="lineno">   12</span>&#160;        return x</div></div><!-- fragment --><p>At last, this model is trained from scratch. A simple gradient descent is used, and the loss function is L2 distance. The training process is asynchronous for efficiency. jittor calculates the gradients and applies graph- and operator-level optimizations via <b>unify IR graph</b> and <b>jit analyzer</b>. In this example, multiple optimizations can be used, including: <b>operator fusion</b>, the activation function and loss function can be fused into the first and second linear layers; Three meta-operators in matrix multiplication could also be fused. <b>Parallelism</b>, it can improve performance of compute-intensive operations on modern multi-core CPUs and GPUs. The operator fusion is a graph-level optimization, and parallelism can be achieved in both graph- and operator-level.</p>
<div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;np.random.seed(0)</div><div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;jt.set_seed(3)</div><div class="line"><a name="l00003"></a><span class="lineno">    3</span>&#160;n = 1000</div><div class="line"><a name="l00004"></a><span class="lineno">    4</span>&#160;batch_size = 50</div><div class="line"><a name="l00005"></a><span class="lineno">    5</span>&#160;base_lr = 0.05</div><div class="line"><a name="l00006"></a><span class="lineno">    6</span>&#160;# we need to stop grad of global value to prevent memory leak</div><div class="line"><a name="l00007"></a><span class="lineno">    7</span>&#160;lr = jt.float32(base_lr).name(&quot;lr&quot;).stop_grad()</div><div class="line"><a name="l00008"></a><span class="lineno">    8</span>&#160;</div><div class="line"><a name="l00009"></a><span class="lineno">    9</span>&#160;def get_data(n):</div><div class="line"><a name="l00010"></a><span class="lineno">   10</span>&#160;    for i in range(n):</div><div class="line"><a name="l00011"></a><span class="lineno">   11</span>&#160;        x = np.random.rand(batch_size, 1)</div><div class="line"><a name="l00012"></a><span class="lineno">   12</span>&#160;        y = x*x</div><div class="line"><a name="l00013"></a><span class="lineno">   13</span>&#160;        yield jt.float32(x), jt.float32(y)</div><div class="line"><a name="l00014"></a><span class="lineno">   14</span>&#160;</div><div class="line"><a name="l00015"></a><span class="lineno">   15</span>&#160;model = Model()</div><div class="line"><a name="l00016"></a><span class="lineno">   16</span>&#160;learning_rate = 0.1</div><div class="line"><a name="l00017"></a><span class="lineno">   17</span>&#160;optim = nn._SGD (model.parameters(), learning_rate)</div><div class="line"><a name="l00018"></a><span class="lineno">   18</span>&#160;</div><div class="line"><a name="l00019"></a><span class="lineno">   19</span>&#160;for i,(x,y) in enumerate(get_data(n)):</div><div class="line"><a name="l00020"></a><span class="lineno">   20</span>&#160;    pred_y = model(x)</div><div class="line"><a name="l00021"></a><span class="lineno">   21</span>&#160;    loss = ((pred_y - y)**2)</div><div class="line"><a name="l00022"></a><span class="lineno">   22</span>&#160;    loss_mean = loss.mean()</div><div class="line"><a name="l00023"></a><span class="lineno">   23</span>&#160;    optim.step (loss_mean)</div><div class="line"><a name="l00024"></a><span class="lineno">   24</span>&#160;    print(f&quot;step {i}, loss = {loss_mean.data.sum()}&quot;)</div><div class="line"><a name="l00025"></a><span class="lineno">   25</span>&#160;</div><div class="line"><a name="l00026"></a><span class="lineno">   26</span>&#160;assert loss_mean.data &lt; 0.005</div></div><!-- fragment --> </div></div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.11
</small></address>
</body>
</html>
