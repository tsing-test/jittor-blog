<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.11"/>
<title>Jittor: Jittor: a Just-in-time(JIT) deep learning framework</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
  $(document).ready(function() { init_search(); });
</script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">Jittor
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.11 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li><a href="index.html"><span>Main&#160;Page</span></a></li>
      <li class="current"><a href="pages.html"><span>Related&#160;Pages</span></a></li>
      <li><a href="annotated.html"><span>Classes</span></a></li>
      <li><a href="files.html"><span>Files</span></a></li>
      <li>
        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="search/mag_sel.png"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.png" alt=""/></a>
          </span>
        </div>
      </li>
    </ul>
  </div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">Jittor: a Just-in-time(JIT) deep learning framework </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h1>Jittor: 即时编译深度学习框架</h1>
<p><a href="#quickstart">Quickstart</a> | <a href="#install">Install</a> | <a href="#tutorial">Tutorial</a> <a href="#快速开始">快速开始</a> | <a href="#安装">安装</a> | <a href="#教程">教程</a></p>
<p>Jittor is a high-performance deep learning framework based on JIT compiling and meta-operators. The whole framework and meta-operators are compiled just-in-time. A powerful op compiler and tuner are integrated into Jittor. It allowed us to generate high-performance code with specialized for your model.</p>
<p>Jittor 是一个基于即时编译和元算子的高性能深度学习框架，整个框架在即时编译的同时，还集成了强大的Op编译器和调优器，为您的模型生成定制化的高性能代码。</p>
<p>The front-end language is Python. Module Design is used in the front-end, like PyTorch and Keras. The back-end is implemented py high performance language, such as CUDA,C++. Jittor前端语言为Python。前端使用了模块化的设计，类似于PyTorch，Keras，后端则使用高性能语言编写，如CUDA，C++。</p>
<p>The following example shows how to model a two-layer neural network step by step and train from scratch In a few lines of Python code. 下面的代码演示了如何一步一步使用Python代码，从头对一个双层神经网络建模。 </p><div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;import jittor as jt</div><div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;from jittor import Module</div><div class="line"><a name="l00003"></a><span class="lineno">    3</span>&#160;from jittor import nn</div><div class="line"><a name="l00004"></a><span class="lineno">    4</span>&#160;class Model(Module):</div><div class="line"><a name="l00005"></a><span class="lineno">    5</span>&#160;    def __init__(self):</div><div class="line"><a name="l00006"></a><span class="lineno">    6</span>&#160;        self.layer1 = nn.Linear(1, 10)</div><div class="line"><a name="l00007"></a><span class="lineno">    7</span>&#160;        self.relu = nn.Relu() </div><div class="line"><a name="l00008"></a><span class="lineno">    8</span>&#160;        self.layer2 = nn.Linear(10, 1)</div><div class="line"><a name="l00009"></a><span class="lineno">    9</span>&#160;    def execute (self,x) :</div><div class="line"><a name="l00010"></a><span class="lineno">   10</span>&#160;        x = self.layer1(x)</div><div class="line"><a name="l00011"></a><span class="lineno">   11</span>&#160;        x = self.relu(x)</div><div class="line"><a name="l00012"></a><span class="lineno">   12</span>&#160;        x = self.layer2(x)</div><div class="line"><a name="l00013"></a><span class="lineno">   13</span>&#160;        return x</div><div class="line"><a name="l00014"></a><span class="lineno">   14</span>&#160;</div><div class="line"><a name="l00015"></a><span class="lineno">   15</span>&#160;def get_data(n): # generate random data for training test.</div><div class="line"><a name="l00016"></a><span class="lineno">   16</span>&#160;    for i in range(n):</div><div class="line"><a name="l00017"></a><span class="lineno">   17</span>&#160;        x = np.random.rand(batch_size, 1)</div><div class="line"><a name="l00018"></a><span class="lineno">   18</span>&#160;        y = x*x</div><div class="line"><a name="l00019"></a><span class="lineno">   19</span>&#160;        yield jt.float32(x), jt.float32(y)</div><div class="line"><a name="l00020"></a><span class="lineno">   20</span>&#160;</div><div class="line"><a name="l00021"></a><span class="lineno">   21</span>&#160;model = Model()</div><div class="line"><a name="l00022"></a><span class="lineno">   22</span>&#160;learning_rate = 0.1</div><div class="line"><a name="l00023"></a><span class="lineno">   23</span>&#160;optim = nn.SGD(model.parameters(), learning_rate)</div><div class="line"><a name="l00024"></a><span class="lineno">   24</span>&#160;</div><div class="line"><a name="l00025"></a><span class="lineno">   25</span>&#160;for i,(x,y) in enumerate(get_data(n)):</div><div class="line"><a name="l00026"></a><span class="lineno">   26</span>&#160;    pred_y = model(x)</div><div class="line"><a name="l00027"></a><span class="lineno">   27</span>&#160;    loss = ((pred_y - y)**2)</div><div class="line"><a name="l00028"></a><span class="lineno">   28</span>&#160;    loss_mean = loss.mean()</div><div class="line"><a name="l00029"></a><span class="lineno">   29</span>&#160;    optim.step (loss_mean)</div><div class="line"><a name="l00030"></a><span class="lineno">   30</span>&#160;    print(f&quot;step {i}, loss = {loss_mean.data.sum()}&quot;)</div></div><!-- fragment --><h2>Contents</h2>
<ul>
<li><a href="#quickstart">Quickstart</a></li>
<li><a href="#install">Install</a></li>
<li><a href="#tutorial">Tutorial</a></li>
<li><a href="#contributing">Contributing</a></li>
<li><a href="#theteam">The Team</a></li>
<li><a href="#license">License</a></li>
</ul>
<h2>大纲</h2>
<ul>
<li><a href="#quickstart">Quickstart</a></li>
<li><a href="#install">Install</a></li>
<li><a href="#tutorial">Tutorial</a></li>
<li><a href="#contributing">Contributing</a></li>
<li><a href="#theteam">The Team</a></li>
<li><a href="#license">License</a></li>
</ul>
<h2>Quickstart</h2>
<p>We provide some jupyter notebooks to help you quick start with Jittor.</p>
<ul>
<li>Example: Model definition and training</li>
<li>Basics: Op, Var</li>
<li>Meta-operator: Implement your own convolution with Meta-operator</li>
</ul>
<h2>Install</h2>
<p>Jittor is written in Python and C++. It requires a compiler for JIT compilation, Currently, we support four compilers:</p><ul>
<li>CPU compiler (require at least one of the following)<ul>
<li>g++ (&gt;=5.4.0)</li>
<li>clang (&gt;=8.0) recommend</li>
</ul>
</li>
<li>GPU compiler (optional)<ul>
<li>nvcc (&gt;=10.0)</li>
</ul>
</li>
</ul>
<p>We provide single line command for quick installation the latest version of Jittor(Ubuntu&gt;=16.04): </p><div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;# install with clang and cuda</div><div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;git clone https://git.net9.org/cjld/jittor.git &amp;&amp; with_clang=1 with_cuda=1 bash ./jittor/script/install.sh</div><div class="line"><a name="l00003"></a><span class="lineno">    3</span>&#160;# install with clang</div><div class="line"><a name="l00004"></a><span class="lineno">    4</span>&#160;git clone https://git.net9.org/cjld/jittor.git &amp;&amp; with_clang=1 bash ./jittor/script/install.sh</div><div class="line"><a name="l00005"></a><span class="lineno">    5</span>&#160;# install with g++ and cuda</div><div class="line"><a name="l00006"></a><span class="lineno">    6</span>&#160;git clone https://git.net9.org/cjld/jittor.git &amp;&amp; with_gcc=1 with_cuda=1 bash ./jittor/script/install.sh</div><div class="line"><a name="l00007"></a><span class="lineno">    7</span>&#160;# install with g++</div><div class="line"><a name="l00008"></a><span class="lineno">    8</span>&#160;git clone https://git.net9.org/cjld/jittor.git &amp;&amp; with_gcc=1 bash ./jittor/script/install.sh</div></div><!-- fragment --><p> After execution, the script will show some environment variables you need to export.</p>
<p>If you use Jittor for CPU computing, we strongly recommend clang(&gt;=8.0) as the back-end compiler of Jittor. Because some customized optimizations will be enabled.</p>
<p>We will show how to install Jittor in Ubuntu 16.04 step by step, Other Linux distributions may have similar commands.</p>
<p>### Step 1: Choose your back-end compiler </p><div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;# g++</div><div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;sudo apt install g++ build-essential libomp-dev</div><div class="line"><a name="l00003"></a><span class="lineno">    3</span>&#160;</div><div class="line"><a name="l00004"></a><span class="lineno">    4</span>&#160;# OR clang-8</div><div class="line"><a name="l00005"></a><span class="lineno">    5</span>&#160;wget -O - https://apt.llvm.org/llvm.sh &gt; /tmp/llvm.sh</div><div class="line"><a name="l00006"></a><span class="lineno">    6</span>&#160;bash /tmp/llvm.sh 8</div></div><!-- fragment --><p> if you choose icc as the back-end compiler, please go to the <a href="https://software.intel.com/en-us/cpp-compiler-developer-guide-and-reference-compiler-setup">offical icc install page</a>.</p>
<h3>Step 2: Install Python and python-dev</h3>
<p>Jittor need python version &gt;= 3.7. </p><div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;sudo apt install python3.7 python3.7-dev</div></div><!-- fragment --><h3>Step 3: Run Jittor</h3>
<p>The whole framework is compiled Just-in-time. Let's install jittor via pip </p><div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;git clone https://git.net9.org/cjld/jittor.git</div><div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;sudo pip3.7 install ./jittor</div><div class="line"><a name="l00003"></a><span class="lineno">    3</span>&#160;export cc_path=&quot;clang-8&quot;</div><div class="line"><a name="l00004"></a><span class="lineno">    4</span>&#160;# if other compiler is used, change cc_path</div><div class="line"><a name="l00005"></a><span class="lineno">    5</span>&#160;# export cc_path=&quot;g++&quot;</div><div class="line"><a name="l00006"></a><span class="lineno">    6</span>&#160;# export cc_path=&quot;icc&quot;</div><div class="line"><a name="l00007"></a><span class="lineno">    7</span>&#160;</div><div class="line"><a name="l00008"></a><span class="lineno">    8</span>&#160;# run a simple test</div><div class="line"><a name="l00009"></a><span class="lineno">    9</span>&#160;python3.7 -m jittor.test.test_example</div></div><!-- fragment --><p> if the test is passed, your Jittor is ready.</p>
<h3>Optional Step 4: Enable CUDA</h3>
<p>Using CUDA in Jittor is very simple, Just setup environment value <code>nvcc_path</code> </p><div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;# replace this var with your nvcc location </div><div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;export nvcc_path=&quot;/usr/local/cuda/bin/nvcc&quot; </div><div class="line"><a name="l00003"></a><span class="lineno">    3</span>&#160;# run a simple cuda test</div><div class="line"><a name="l00004"></a><span class="lineno">    4</span>&#160;python3.7 -m jittor.test.test_cuda </div></div><!-- fragment --><p> if the test is passed, your can use Jittor with CUDA by setting <code>use_cuda</code> flag. </p><div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;import jittor as jt</div><div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;jt.flags.use_cuda = 1</div></div><!-- fragment --><h3>Optional Step 5: Run full tests</h3>
<p>To check the integrity of Jittor, we can run full tests. </p><div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;python3.7 -m jittor.test -v</div></div><!-- fragment --><p> if those tests are failed, please report bugs for us, and feel free to contribute ^_^</p>
<h2>Tutorial</h2>
<p>In the tutorial section, we will briefly explain the basic concept of Jittor.</p>
<p>To train your model with Jittor, there are only three main concepts you need to know:</p><ul>
<li>Var: basic data type of jittor</li>
<li>Operations: Jittor'op is simular with numpy</li>
</ul>
<h3>Var</h3>
<p>Var is the basic data type of jittor. Computation process in Jittor is asynchronous for optimization. If you want to access the data, <code>Var.data</code> can be used for synchronous data accessing.</p>
<div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;import jittor as jt</div><div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;a = jt.float32([1,2,3])</div><div class="line"><a name="l00003"></a><span class="lineno">    3</span>&#160;print (a)</div><div class="line"><a name="l00004"></a><span class="lineno">    4</span>&#160;print (a.data)</div><div class="line"><a name="l00005"></a><span class="lineno">    5</span>&#160;# Output: float32[3,]</div><div class="line"><a name="l00006"></a><span class="lineno">    6</span>&#160;# Output: [ 1. 2. 3.]</div></div><!-- fragment --><p>And we can give the variable a name</p>
<div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;c.name(&#39;c&#39;)</div><div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;print(c.name())</div><div class="line"><a name="l00003"></a><span class="lineno">    3</span>&#160;# Output: c</div></div><!-- fragment --><h3></h3>
<h3>Operations</h3>
<p>First, let's get started with Op. Jittor'op is simular with numpy. Let's try some operations. We create Var <code>a</code> and <code>b</code> via operation <code>jt.float32</code>, and add them. Printing those variables shows they have the same shape and dtype. </p><div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;import jittor as jt</div><div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;a = jt.float32([1,2,3])</div><div class="line"><a name="l00003"></a><span class="lineno">    3</span>&#160;b = jt.float32([4,5,6])</div><div class="line"><a name="l00004"></a><span class="lineno">    4</span>&#160;c = a*b</div><div class="line"><a name="l00005"></a><span class="lineno">    5</span>&#160;print(a,b,c)</div><div class="line"><a name="l00006"></a><span class="lineno">    6</span>&#160;print(type(a), type(b), type(c))</div><div class="line"><a name="l00007"></a><span class="lineno">    7</span>&#160;# Output: float32[3,] float32[3,] float32[3,]</div><div class="line"><a name="l00008"></a><span class="lineno">    8</span>&#160;# Output: &lt;class &#39;jittor_core.Var&#39;&gt; &lt;class &#39;jittor_core.Var&#39;&gt; &lt;class &#39;jittor_core.Var&#39;&gt;</div></div><!-- fragment --><p> Beside that, All the operators we used jt.xxx(Var, ...) have alias Var.xxx(...). For example:</p>
<div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;c.max() # alias of jt.max(a)</div><div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;c.add(a) # alias of jt.add(c, a)</div><div class="line"><a name="l00003"></a><span class="lineno">    3</span>&#160;c.min(keepdims=True) # alias of jt.min(c, keepdims=True)</div></div><!-- fragment --><p>if you want to know all the operation which Jittor supports. try <code>help(jt.ops)</code>. All the operation you found in <code>jt.ops.xxx</code>, can be used via alias <code>jt.xxx</code></p>
<div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;help(jt.ops)</div><div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;# Output:</div><div class="line"><a name="l00003"></a><span class="lineno">    3</span>&#160;#   abs(x: core.Var) -&gt; core.Var</div><div class="line"><a name="l00004"></a><span class="lineno">    4</span>&#160;#   add(x: core.Var, y: core.Var) -&gt; core.Var</div><div class="line"><a name="l00005"></a><span class="lineno">    5</span>&#160;#   array(data: array) -&gt; core.Var</div><div class="line"><a name="l00006"></a><span class="lineno">    6</span>&#160;#   binary(x: core.Var, y: core.Var, op: str) -&gt; core.Var</div><div class="line"><a name="l00007"></a><span class="lineno">    7</span>&#160;#   ......</div></div><!-- fragment --> <h3>More</h3>
<p>If you want to know more about Jittor, please check out the notebooks below:</p>
<ul>
<li>Quickstart<ul>
<li>Example: Model definition and training</li>
<li>Basics: Op, Var</li>
<li>Meta-operator: Implement your own convolution with Meta-operator</li>
</ul>
</li>
<li>Advanced<ul>
<li>Custom Op: write your operator with C++ and CUDA and JIT compile it</li>
<li>Profiler: Profiling your model</li>
<li>Jtune: Tool for performance tuning</li>
</ul>
</li>
</ul>
<p>Those notebooks can be started in your own computer by <code>python3.7 -m jittor.notebook</code></p>
<h2>Contributing</h2>
<p>Jittor is still young. It may contain bugs and issues. Please report them in our bug track system. Contributions are welcome. Besides, if you have any ideas about Jittor, please let us know.</p>
<h2>The Team</h2>
<p>Jittor is currently maintained by Dun Liang, Guo-Ye Yang, Guo-Wei Yang, Wen-Yang Zhou and Meng-Hao Guo. If you are also interested in Jittor and want to improve it, Please join us!</p>
<h2>License</h2>
<p>Jittor is Apache 2.0 licensed, as found in the LICENSE.txt file. </p>
</div></div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.11
</small></address>
</body>
</html>
